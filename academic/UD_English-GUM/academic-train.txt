Aesthetic Appreciation and Spanish Art: 
Insights from Eye-Tracking 
Claire Bailey-Ross claire.bailey-ross@port.ac.uk University of Portsmouth, United Kingdom 
Andrew Beresford a.m.beresford@durham.ac.uk Durham University, United Kingdom 
Daniel Smith daniel.smith2@durham.ac.uk Durham University, United Kingdom 
Claire Warwick c.l.h.warwick@durham.ac.uk Durham University, United Kingdom 
How do people look at and experience art? 
Which elements of specific artworks do they focus on? 
Do museum labels have an impact on how people look at artworks? 
The viewing experience of art is a complex one, involving issues of perception, attention, memory, decision-making, affect, and emotion. 
Thus, the time it takes and the ways of visually exploring an artwork can inform about its relevance, interestingness, and even its aesthetic appeal. 
This paper describes a collaborative pilot project focusing on a unique collection of 17th Century Zurbarán paintings. 
The Jacob cycle at Auckland Castle is the only UK example of a continental collection preserved in situ in purpose-built surroundings. 
While studies of the psychology of art have focused on individual works and distinctions between representative / non-representative topics, no work has been completed on the aesthetic appreciation of collections or of devotional themes. 
In this paper, we report upon the novel insights eye-tracking techniques have provided into the unconscious processes of viewing the unique collection of Zurbarán artworks. 
The purpose of this pilot study was to assess the effects of different written interpretation on the visual exploration of artworks. 
We will discuss the potential implications of these techniques and our understanding of visual behaviours on museum and gallery practice. 
The project brings together established research strengths in Spanish art history, experimental psychology, digital humanities, and museum studies to explore, using eye-tracking techniques, aesthetic reactions to digital representations of the individual Zurbarán artworks as well as the significance of the collection as a whole. 
Our experience of art develops from the interaction of several cognitive and affective processes; the beginning of which is a visual scan of the artwork. 
When regarding an artwork, a viewer gathers information through a series fixations, interspersed by rapid movements of the eye called saccades. 
The direction of saccades is determined by an interaction between the goals of the observer and the physical properties of the different elements of the scene (e.g. colour, texture, brightness etc). 
Importantly, studying eye movements offers an insight that does not depend on the participants’ beliefs, memories or subjective impressions of the artwork. 
Previous eye tracking research has highlighted the potential to transform the ways we understand visual processing in the arts (see for example Brieber 2014; Binderman et al., 2005) and at the same time offers a direct way of studying several important factors of a museum visit (Filippini Fantoni et al., 2013; Heidenreich & Turano 2011; Milekic 2010). 
Zurbarán’s cycle of Jacob and his Sons has been on display in the Long Room at Auckland Castle for over 250 years. 
It is the only cycle to be preserved in purpose-built surroundings in the UK, and one of very few of its kind in the world. 
It has a long history in scholarship (Baron & Beresford 2014), but many key aspects of its production and significance have not yet been fully understood. 
In this study we used eye-tracking in the first stage of exploring audience experience of the extensive Spanish art collections of County Durham, of which the 13 Zurbarán artworks (there are actually only 12 Zurbarán artworks, the 13th Benjamin, is a copy by Arthur Pond) are a key part of, to investigate the ways in which audiences look at Spanish art, how aesthetic experience is evaluated and whether audiences can be encouraged to approach art in different ways. 
This pilot project primarily investigated how participants visually explore artworks and provides new insights into the potential eye-tracking has to transform the ways we understand visual processing in arts and culture and at the same time offer a direct way of studying several important factors of a museum visit, namely to assess the effects of label characteristics on visitor visual behaviour. 
1 Introduction 
Tenured and tenure-track university faculty play a special role in determining the speed and direction of scientific progress, both directly through their research and indirectly through their training of new researchers. 
Past studies establish that each of these efforts is strongly and positively influenced through various forms of faculty diversity, including ethnic, racial, and gender diversity. 
As an example, research shows that greater diversity within a community or group can lead to improved critical thinking [1] and more creative solutions to complex tasks [2, 3] by pairing together individuals with unique skillsets and perspectives that complement and often augment the abilities of their peers. 
Additionally, diversity has been shown to produce more supportive social climates and effective learning environments [4], which can facilitate the mentoring of young scientists. 
Despite these positive effects, however, quantifying the impact of diversity in science remains exceedingly difficult, due in large part to a lack of comprehensive data about the scientific workforce. 
Measuring the composition and dynamics of a scientific workforce, particularly in a rapidly expanding field like computer science, is a crucial first step toward understanding how scholarly research is conducted and how it might be enhanced. 
For many scientific fields, however, there is no central listing of all tenure-track faculty, making it difficult to define a rigorous sample frame for analysis. 
Further, rates of adoption of services like GoogleScholar and ResearchGate vary within, and across disciplines. 
For instance, gender representation in computing is an important issue with broad implications [5], but without a full census of computing faculty, the degree of inequality and its possible sources are difficult to establish [6]. 
Some disciplines, like political science, are organized around a single professional society, whose membership roll approximates a full census [7]. 
Most fields, on the other hand, including computer science, lack a single all-encompassing organization and membership information is instead distributed across many disjoint lists, such as web-based faculty directories for individual departments. 
Because assembling such a full census is difficult, past studies have tended to avoid this task and have instead used samples of researchers [8 – 11], usually specific to a particular field [12 – 16], and often focused on the scientific elite [17, 18]. 
Although useful, such samples are not representative of the scientific workforce as a whole and thus have limited generalizability. 
One of the largest census efforts to date assembled, by hand, a nearly complete record of three academic fields: computer science, history, and business [19]. 
This data set has shed considerable light on dramatic inequalities in faculty training, placement, and scholarly productivity [6, 19, 20]. 
But, this data set is only a single snapshot of an evolving and expanding system and hence offers few insights into the changing composition and diversity trends within these academic fields. 
In some fields, yearly data on faculty numbers and composition are available in aggregate. 
In computer science, the Computing Research Association (CRA) documents trends in the employment of PhD recipients through the annual Taulbee survey of computing departments in North America (cra.org/resources/taulbee-survey). 
Such surveys can provide valuable insight into trends and summary statistics on the scientific workforce but suffer from two key weaknesses. 
First, surveys are subject to variable response rates and the misinterpretation of questions or sample frames, which can inject bias into fine-grained analyses [21, 22]. 
Second, aggregate information provides only a high-level view of a field, which can make it difficult to investigate causality [23]. 
For example, differences in recruitment and retention strategies across departments will be washed out by averaging, thereby masking any insights into the efficacy of individual strategies and policies. 
Here, we present a novel system, based on a topical web crawler, that can quickly and automatically assemble a full census of an academic field using digital data available on the public World Wide Web. 
This system is efficient and accurate, and it can be adapted to any academic discipline and used for continuous collection. 
The system is capable of collecting census data for an entire academic field in just a few hours using off-the-shelf computing hardware, a vast improvement over the roughly 1600 hours required to do this task by hand [19]. 
By assembling an accurate census of an entire field from online information alone, this system will facilitate new research on the composition of academic fields by providing access to complete faculty listings, without having to rely on surveys or professional societies. 
This system can also be used longitudinally to study how the workforce’s composition changes over time, which is particularly valuable for evaluating the effectiveness of policies meant to broaden participation or improve retention of faculty. 
Finally, applied to many academic fields in parallel, the system can elucidate scientists’ movement between different disciplines and relate those labor flows to scientific advances. 
In short, many important research questions will benefit from the availability of accurate and frequently-recollected census data. 
Our study is organized as follows. 
We begin by detailing the design and implementation of our web crawler framework. 
Next, we present the results of our work in two sections. 
The first demonstrates the validity and utility of the crawler by collecting census data for the field of computer science and comparing it to a hand-curated census, collected in 2011 [19]. 
The second provides an example of the type of research enabled by our system and uses the 2011 and 2017 censuses to investigate the “leaky pipeline” problem in faculty retention. 
2. New Institutional Economics: Theoretical Foundations and Application to the Argentine Case 
2.1. New Institutional Economics, Property Rights, and Credibility of the State’s Commitment 
Throughout the second half of the 20th century, we have witnessed the return of institutions to economic analysis. 
The awarding of the Nobel Prize in Economics to its main representatives (Douglass North, Ronald Coase, Oliver Williamson, and Elinor Ostrom) has contributed to its greater recognition. 
The two main notions of this economic approach are the concepts of transaction costs and institutions, analyzed by Coase and North, respectively. 
Coase [13,14] argues that economic transactions involve costs, and where these costs outweigh the gains, the exchange will not take place. 
For its part, North defined the institutions as the “rules of the game”: they determine the structure of the economy, establish incentives for economic behavior, and affect social interaction [12]. 
Thus, institutions also determine the level of uncertainty to which individuals are subject, stimulating or discouraging transactions. 
A viable economy requires an institutional structure that reduces existing uncertainty and guarantees property rights. 
In other words, it is critical that economic agents believe that their property rights will not be taken away by other public or private actors [15]. 
Achieving these objectives requires the creation of inclusive institutions, which guarantee the right to private property, incorporate an impartial legal system, and promote a society based on equality of conditions [16]. 
Of course, the state takes a lead role in promoting this type of institution. 
The state must protect property rights, provide public services, and ensure a sound judiciary. 
It is responsible also for imposing order and promoting a climate of cooperation among agents, penalizing all violators of contracts [17]. 
The path to a prosperous society is not an easy one. 
It is society that establishes, through political processes, the type of economic institutions that are adopted. 
If there are disagreements about the policies to be implemented, then the group that wins the political game will make the final decision. 
On the one hand, a society that adopts policies promoting inclusive political institutions will develop pluralist and centralized institutions. 
Inclusive institutions guarantee the right to private property, to an impartial legal system and promote a society based on equal conditions. 
These institutions benefit not only the elites, but society as a whole. 
The consequences of implementing inclusive institutions are reflected in increased activity, productivity, and economic growth. 
On the other hand, if extractive political institutions are implemented, then power will be consolidated within a small number of groups. 
In governments with extractive policies, the group in power usually extracts resources from the rest of the population for its own enrichment and well-being. 
Another feature of this type of government is that powerful interest groups oppose increased pluralism because it typically results in the loss of their privileges. 
Those in power have little interest in their power devolving to a greater number of agents, as would occur under political institutions that were more pluralistic [16]. 
It follows that the role of the state is essential. 
The institutional structure of a state and its constitution are responsible for restricting predatory action—even by the state itself—and for creating rules that benefit the community [18]. 
Moreover, compliance mechanisms must reflect ex ante and ex post standards [12]. 
But what happens when property rights are not respected? 
What if it is the state itself that exhibits predatory behavior? 
All these questions have a place in this case analysis. 
The expropriation of YPF is a case in which the Argentine State presents predatory behavior, encouraged by an extractive institutional framework. 
If the decisions taken by the state are not based on the general interest and, in their execution, do not respect property rights, the number of transactions will be reduced, which will result in unfavorable economic performance. 
In a scenario with insecure property rights, asymmetric information, and a judicial system that acts as a lax enforcement mechanism, as discussed in the following sections, the FDI is difficult to attract. 
2.2. Institutional Structure of the Argentine State 
In order to understand policymaking and its effects, one must have adequate knowledge of the focal country’s institutional framework. 
Most of Argentina’s governments have been of the predatory type. 
The country’s unequal economic growth originated in the colonial era and reflects how the Spanish metropolis influenced the establishment of extractive institutions [7]. 
Argentine politics are therefore highly unstable. 
Achieving political order would require the government to limit its actions and guarantee the rights of citizens—neither of which has yet occurred in Argentina. 
Distant Rhythm: Automatic Enjambment Detection on Four Centuries of Spanish Sonnets 
Pablo Ruiz Fabo 
pabloruizfabo@gmail.com 
Lattice Lab, CNRS, France 
Clara Martínez Cantón 
cimartinez@flog.uned.es 
Universidad Nacional de Educación a Distancia, Spain 
Thierry Poibeau 
thierry.poibeau@ens.fr 
Lattice Lab, CNRS, France 
Introduction 
Enjambment takes place when a syntactic unit is broken up across two lines of poetry (Domínguez Caparrós, 2000: 103), giving rise to different stylistic effects (e.g. increased emphasis on elements of the broken-up phrase, or contrast between those elements), or creating double interpretations for the enjambed lines (García-Paje, 1991). 
In Spanish poetry, the syntactic configurations under which enjambment takes place have been described extensively, and detailed studies on the use of enjambment by individual authors exist (see Martínez Cantón, 2011 for an overview) including, among others Quilis (1964), Domínguez Caparrós, (2000), Paraíso, (2000), Spang (1983) for a description of enjambment, and Alarcos (1966), Senabre (1982), Luján (2006), Martínez Fernández (2010) for case-studies on a single author. 
However, a larger-scale study to identify enjambment across hundreds of authors spanning several centuries, enabling distant reading (Moretti, 2013), was not previously available. 
Given that need, we have developed software, based on Natural Language Processing, that automatically identifies enjambment in Spanish, and applied it to a corpus of approx. 3750 sonnets by ca. 1000 authors, from the 15th to the 19th century. 
What is the interest of such large-scale automatic analyses of enjambment? 
First, the literature shows a debate about which specific syntactic units can be considered to trigger enjambment, if split across two lines, and whether lexical and syntactic criteria are sufficient to identify enjambment. 
Second, the stylistic effects that enjambment permits are also an object of current research (Martínez Fernández, 2010). 
Systematically collecting large amounts of enjambment examples provides helpful evidence to assess scholars’ current claims, and may stimulate novel analyses. 
Finally, our study complements Navarro’s (2016) automatic metrical analyses of Spanish Golden Age sonnets, by covering a wider period and focusing on enjambment. 
The abstract is structured thus: 
First we provide the definition of enjambment adopted. 
Then, our corpus and system are described, followed by an evaluation of the system. 
Finally, findings on enjambment in our diachronic sonnet corpus are discussed. 
The project’s website provides details omitted here for space reasons, including samples for the corpus, results, and other details. 
Enjambment in Spanish 
Syntactic and metrical units often match in poetry. 
However, this trend has been broken since antiquity for various reasons (Parry (1929) on Homer, or Flores Gómez (1988) on early classical poetry). 
In Spanish tradition, enjambment (in Spanish, "encabalgamiento") is considered to take place when a pause suggested by poetic form (e.g. at the end of a line or across hemistichs) occurs between strongly connected lexical or syntactic units, triggering an unnatural cut between those units. 
Quilis (1964) performed poetry reading experiments, proposing that the following strongly connected elements give rise to enjambment, should a poetic-form pause break them up: 
Lexical enjambment: Breaking up a word. 
We translated "lexical enjambment" from Quilis’s terms "encabalgamiento léxico" or "tmesis". 
Phrase-bounded enjambment: Within a phrase, breaking up sequences like "noun + adjective", "verb + adverb", "auxiliary verb + main verb", among others. 
We translated "phrase-bounded enjambment" from "encabalgamiento sirremático". 
Cross-clause enjambment: Between a noun antecedent and the pronoun heading the relative clause that complements the antecedent. 
We translated "cross-clause enjambment" from Quilis’s "encabalgamiento oracional". 
The project site includes Quilis’s complete list of syntactic environments that can trigger enjambment, as well as the types identified by our system. 
Besides the enjambment types above, Spang (1983) noted that if a subject or direct object and their related verbs occur in two different lines of poetry, this can also feel unusual for a reader, even if the effect is less pronounced than in the environments identified by Quilis. 
To differentiate these cases from enjambment proper, Spang calls these cases "enlace", translated here as "expansion". 
Quilis (1964) was the only author so far to gather recitation-based experimental evidence on enjambment. 
His typology is still considered current, and was adopted by later authors, although complementary enjambment typologies have been proposed, as Martínez Cantón (2011) reviews. 
Our system identifies Quilis’ types, besides Spang’s expansion cases. 
Epistemic pollution 
Agents can rationally choose between experts only if the criteria that distinguish genuine experts from charlatans are common sense or widely known: 
if agents are to satisfy the epistemic conditions on responsibility, they must know what kinds of knowledge they must utilize to guide their selection of sources (on pain of infinite regress). 
In fact, many, if not all, the markers of expertise identified by philosophers enjoy widespread recognition. 
The fact that these criteria are widely known, however, offers an opportunity to those who would use them for deception, witting or unwitting. 
Since expertise must be assessed through indirect markers, to mimic the markers of expertise is to mimic expertise [17]. 
We live in an epistemic environment that is heavily and deliberately polluted by agents who use mimicry and other methods as a means of inflating their pretense to expertise. 
This fact, together with the fact that such deception is widely known to occur, reduces ordinary people’s trust in expert authority and diminishes their capacity to distinguish reliable from unreliable sources. 
For instance, those with an interest in deceiving the general public may set up parallel institutions that ostensibly guarantee expertise, taking advantage of the ways in which these parallel institutions mimic legitimate institutions to ensure that people are taken in. 
There are some egregious examples of this practice in the field of health care. 
For example, a small number of doctors set up the American College of Pediatricians (ACPeds) to advocate socially conservative viewpoints related to child health care. 
Such an organization is surely permissible, but it has had the unfortunate (and likely intended) effect of muddying debates in the public forum by misleading people into thinking that the college speaks for the pediatric profession at large. 
Thus, when ACPeds issued a statement condemning gender reassignment surgery in 2016 [21], many people mistook the organization’s political beliefs for the consensus view among United States pediatricians — although the peak body for pediatric workers, the American Academy of Pediatrics, has a much more positive view of gender dysphoria [22]. 
Insofar as the larger organization, with a broader membership base, can be expected to reflect a wider range of expert opinions and a higher degree of expertise, it is reasonable to give its views greater weight than those of the smaller organization. 
When ACPeds allows or encourages the impression that it speaks for the profession, it introduces an epistemic pollutant. 
A yet more egregious example of such pollution involved collaborative efforts by pharmaceutical companies and the publishing giant Elsevier to produce publications mimicking peer-reviewed journals in the interest of promoting the companies’ commercial products [23]. 
The companies hoped to leverage the prestige of Elsevier with these fake journals to endow their promotional “research” with an air of reliability. 
When the deceit was uncovered, however, the effect was just the opposite: 
the legitimacy of the published findings was not enhanced through their publication by Elsevier, but rather the legitimacy of Elsevier’s publications — and, by extension, all academic journals — was diminished through their dissemination of deceptive and commercially interested research. 
More recently, institutions of academic expertise have been subject to a large and growing outbreak of so-called predatory journals — journals that will publish almost anything for a fee. 
Once again, this phenomenon has the effect of making peer-reviewed journals appear less legitimate. 
At times, even those who work in academia may be unsure of a particular journal’s legitimacy, and there are genuine borderline cases. 
For example, the Frontiers contingent of journals appears legitimate — at least to me — despite the fact that authors are expected to pay a publication fee. 8 
Yet some Frontiers journals appear to have engaged in bad behavior, whether for profit or for some other motive. 
Frontiers in Public Health controversially published articles linking vaccines and autism [24] and questioning the link between HIV and AIDS [25]. 
Whether due to this behavior or not, Jeffrey Beall decided to add the publisher to his influential (but now sadly unavailable) list of questionable journals [26]. 
The controversy surrounding Beall’s decision indicates how difficult it is to make such judgments — even for professionals. 
If academics with expertise in relevant fields have difficulty assessing whether particular journals or particular publishers are legitimate, one cannot reasonably expect ordinary people to make such judgments. 
If their confidence in scientific findings is lowered across the board as the result of such epistemic pollution, one can hardly blame them. 
Since conflicts of interest are a reason to discount expertise, it is incumbent on me to note that I have published in Frontiers journals on several occasions. 
Epistemic pollution may stem not only from counterfeit institutions of knowledge production but also from bad behavior by legitimate institutions. 9 
For example, pollution may result from attempts to game the systems put in place to track expertise. 
Consider institutions with a credentialing function, such as universities, bar associations, or peer review bodies. 
These institutions do not exist solely to credential experts. 
They have other functions, and these functions may come into conflict, creating pressures to inflate credentials. 
For example, universities have a financial incentive to inflate the expertise of their academic staff, thereby increasing their rankings, bringing in grant money, and attracting students. 
Systems that assess expertise can be manipulated, and many cases of such manipulation exist — take the recent example by the University of Malaysia, which attempted to boost metrics by urging its faculty to cite one another [28]. 
For this reason, institutions may also be slow to investigate accusations of fraud, and they may try to keep their discoveries in-house to protect their reputations. 
The Use of Cognitive Digital Games in School: 
Contributions to Attention 
Daniela Karine Ramos dadaniela@gmail.com Universidade Federal de Santa Catarin 
Bruna Anastacio brunaanastacio@hotmail.com Universidade Federal de Santa Catarin 
Cognitive games involve a number of different games working aspects of human cognition, while proposing the intersection between the sets of concepts, fun and cognition, for the improvement of cognitive functions. 
The attention is the main point made in this study, since it is fundamental to the learning process and be recurring complaint among parents and teachers in schools. 
With respect to the contributions of digital games to improvement of cognitive processes, researchers suggest that regular practice has a significant influence on improving the performance related to basic visual skills (Li, Polat, Scalzo, & Bavelier, 2010); on the ability to perceive objects simultaneously (Dye & Bavelier, 2010; Feng, Spence, & Pratt, 2007); and on the ability to do more than one task at the same time (Boot, Kramer, Simons, Fabiani, & Gratton, 2008). 
Other studies specifically investigate the use of digital games in the school context and suggest potential for digital game use to improve of student's attention span at preschool age (Rueda, Checa, & Cómbita, 2012), to improve overall intelligence capacity of elementary school children (Miller & Robertson, 2010), and to better performance of working memory ability (Klingberg et al., 2005; Thorell, Lindqvist, Nutley, Bohlin, & Klingberg, 2009). 
Considering the importance of the proper functioning of attention, because of its involvement in the regulation of thoughts and emotions, maintaining the performance of this process is very important, especially in school, where the child must acquire content in an environment full of countless distractors. 
The study in question focuses on the attention, proposing and evaluation in the context of the classroom. 
Thus, it suggests the use of digital games in an integrated way the school activities in the classroom. 
The games have features like increasing challenges, rules that establish what can and cannot be done, and involvement of the player in the quest to gain skills and win the game (Kirriemuir & McFarlane, 2004; Prensky, 2005). 
We aim to investigate the contributions of the use of a system that integrates cognitive digital games to a database, of the Escola do Cérebro, for monitoring and improvement of cognitive skills, highlighting the attention. 
The games involve challenges and rules involving the exercise of cognitive functions, especially the working memory, attention and capacity of solving problems. 
The study combines qualitative and quantitative approaches. 
It collects the data based on the observation of the proposed interventions as well as interviews conducted with participating teachers and students to identify their perceptions of digital games’ contributions to the learning process. 
Furthermore, before and after the implementation of the intervention, we performed a D2 Test of attention that measures selective and sustained attention, as well as visual scanning accuracy and speed. 
The intervention consisted in the use of the Escola do Cérebro, using tablets in the classroom, daily for a period of five weeks. 
The sample consisted of 71 students of the Application School of Basic Education, Federal University of Santa Catarina, aged 7 and 9 years old (M = 7.64 ± 1.12), which were divided into two groups: participant and control. 
The first (n = 31) participated in the intervention, the control group (n = 40) was only evaluated using the test before and after the same time interval of interventions. 
The Escola do Cérebro is a platform that integrates seven digital games into a database. 
The application allows visualization of the player's performance and offers the possibility of monitoring by teachers. 
Students have their scores measured by four variables: time, speed, stability and accuracy. 
A statistical analysis was performed based on the application of the paired t-test on the difference of the overall score obtained in the test before and after the intervention in the two groups. 
The difference in the results obtained from the application of D2 Test of attention before and after was statistically significant (p < 0.05), the participant group had mean and standard deviation 60.23 (64.75) respectively, while the control group was 20.00 (42.65). 
The result indicates significant improvement in the performance of the sustained attention in the test, as well as a high dispersion, which reveals a variation in relation to the performance. 
In addition, students participating in the interview reported a preference for games that involve problem solving, recognize the need to plan actions in relation to their importance for the game and for daily activities, and realize improvements in the ability to sustain attention. 
The teachers observed changes after the intervention, emphasizing the greater persistence and involvement in school activities, and in some students, improvement in the ability to sustain attention. 
From this, we conclude that an intervention based on cognitive digital games offers contributions to the learning process and improvement of sustained attention. 
Introduction 
A fundamental tenet of linguistic science is that the sound of a word has a purely arbitrary connection to the word's meaning [1], [2]. 
Thus, the sound of the word dog in English is connected to the concept ‘dog’ by historical accident and not by any natural connection; roughly the same concept is just as well denoted in French by chien, in German by hund, and in Japanese by inu. 
But it is not that a word can have just any vocal sound. 
While the possibility space for sound systems of the world's language is enormous, any given language makes use of only a restricted portion of the possible sounds [3], [4]. 
It follows from these two basic principles – the ‘arbitrariness of the sign’, and the ‘selectiveness of particular sound systems’ – that the words that exist in the world's languages should sound quite different from each other, and that the likelihood that there are universal words is extremely small. 
But in this study we present a striking exception to this otherwise robust rule. 
From a systematic comparison of 10 spoken languages from 5 continents we find evidence suggesting that a word like ‘Huh?’ – used as a ‘repair initiator’ when, for example, one has not clearly heard what someone just said [5], [6] – is a universal word. 
There are two distinct claims being made here: 1. that Huh? is universal, and 2. that Huh? is a word. 
In support of the first claim, we show that the similarities in form and function of an interjection with the specific function of repair initiation are very much greater across languages than chance coincidence would admit. 
In fact the variation in form in unrelated languages across the globe is about the same as the variation we find in the way any regular word (e.g., dog) is pronounced across dialects of English. 
In support of the second claim, we show that Huh? meets the criteria of a word in the sense of being a conventional lexical sign which must be learnt. 
Thus, in contrast to what has been presumed for interjections in general [7], [8] and for huh? in particular [9], [10], we find that this item is linguistic in nature rather than being a mere grunt or non-lexical sound. 
We show that the form is locally calibrated in ways that show it fitting within different language systems. 
Huh? may be a non-prototypical word, but it is a word. 
Finally, we address the question of why all languages should have such a word and why its form should be so similar across languages. 
We observe that this item fulfils a crucial need shared by all languages – the efficient signalling of problems of hearing and understanding – and we propose that its form is constrained by selective pressures in a conversational environment that is essentially the same in all languages. 
Consider a case from English [10]: 
Extract 1 American English [NB, 1:1:19] 
After speaker G makes a statement, speaker E utters the interjection huh?. 
This is followed by a repetition of the original statement by G. 
The technical term for this type of sequence is “open other-initiated repair”: 
repair is initiated not by the speaker of the first turn but by the other participant (“other-initiated”), and the repair initiator signals that there is a problem, but it leaves open what the problem is (“open”) [11]. 
The actual repair operation in response to this interjection is usually simply repetition, sometimes with slight modification. 
Extracts 2 and 3 show structurally identical sequences in two other languages: Siwu, a Kwa language spoken in Ghana, and Lao, a Tai-Kadai language spoken in Laos. 
Extract 2 Siwu (Ghana) [Maize1_1017013] 
Extract 3 Lao (Laos) [CONV_050815c_03.10] 
These examples show that it is possible to identify the same conversational structure in unrelated languages. 
Essentially, this method gives us a natural control over conversational data, making possible systematic comparison across languages [12], [13]. 
Sequences of other-initiated repair have been identified in every spoken language investigated so far [14], [15], and as the examples show, the interjection in the pivotal turn can be remarkably similar. 
This leads to the question driving our study: 
is huh? in this context a universal word? 
By compiling data from published literature we found that in thirty-one languages around the world, the interjection for other-initiated repair appears to be strongly similar (Figure 1). 
However, written sources are rarely explicit about the precise form, meaning, and use of interjections. 
The most reliable way to study a conversational interjection is by examining cases of actual use. 
Therefore we collected data from recordings of naturally occurring informal conversations in a sample of 10 languages from 5 continents, varying fundamentally in terms of phonology, word structure, and grammar (languages 1–10 in Figure 1). 
For optimal comparability, we studied the exact same conversational environment across languages: that of other-initiated repair (OIR), in which one participant produces a turn at talk, the other then signals some trouble with this turn, and finally the first produces a next turn which aims to solve the trouble, usually by means of repetition and/or modification. 
In some languages the interjection, or an item similar to it, was also found in other sequential environments, for instance to mark surprise or to pursue a response. 
Such alternative (and probably derived) uses provide insight in possible paths of semantic change, but we exclude them here to make sure we are comparing like with like. 
4. Discussion. 
We hypothesized that the children, who were over the age of six at the time of collection, would have adult-like SI interpretations. 
We expected that the use of a training session and then the format of the question would guide participants towards evaluating pragmatic felicity over providing truth value judgments, and would result in adult-like interpretations. 
Further, we posited that if our initial hypothesis was not supported, and in fact there was some non-adult-like performance, that there would be significant variability among the conditions, with cardinal numbers having the highest performance and some the weakest. 
On the surface, it does seem as if there is variability and perhaps a larger sample size or more items could lead to a significant finding (see limitations below), but at least for this study, the only significant finding was the difference between cardinal numbers and some. 
Significant differences were not observed between all and cardinal numbers or all and some. 
4.1. LIMITATIONS AND FUTURE DIRECTIONS. 
This study is limited in its power and generalizability. 
It serves as a reminder of the work that needs to be completed for the cross-linguistic study of SI in particular and acquisition more generally. 
Bantu languages are not included in studies of acquisition often enough despite the numerous languages in this linguistic grouping and the millions of people using them. 
Future studies need to include more participants and more items to increase power. 
Future studies should also include a larger age range of children to document the age at which adult-like performance emerges. 
Related cross-cultural studies have resulted in insufficient statistical power, but interesting trends (e.g., Nedwick, 2014). 
Follow-up studies with increased stimuli and participants are needed. 
In a future study, it is also important to consider if additional training in the difference between reporting on felicity and truth value judgments would impact results, or if more naturalistic experimental conditions (e.g., demonstrating actions using real objects instead of two-dimensional depictions) would be beneficial. 
Previous studies in this region of sub-Saharan Africa have found evidence of cultural differences in testing behavior (Hein, Reich, Marks, Thuma, & Grigorenko, 2016). 
The current study is too small to make strong conclusions with regard to cultural differences and experimental methods; 
however, in the study by Hein and colleagues it was found that children responded more or less frequently based on factors such as stimuli type. 
In the current study, the responses from two children were not included because they responded the same way to every item through the training and all test items. 
More specifically, they answered “yes” that the puppet described the pictures well. 
It is possible that these two children understood the task and really did believe that the puppet did not produce any poor descriptions, and in this regard, are not yet adult-like in their SI interpretations. 
This is unlikely as they said that even the first training item was said well. 
It is also possible though that these two children were attempting to please the data collectors, or be polite, and that “yes” was in some ways a default answer to be provided when having to respond verbally in a test context. 
Additional training with the methodology could improve outcomes. 
The puppets used were picked specifically for this task with careful attention to their appearance. 
Upon arrival at the school, however, it became clear that the children were not accustomed to playing with puppets and that the data collection would be a novel experience for them. 
Further exploration with greater cardinal number ranges could also prove interesting. 
Numbers through five in the participating communities are most often expressed with native Chitonga words while numbers greater than five are typically indicated using English borrowings. 
The cardinal numbers included in this study were only one through five in order to avoid additional item variability, but larger numbers should be included in future research. 
5. Conclusion. 
Albeit limited, these results provide valuable insight into SI interpretation by Chitonga-speaking children and demonstrate that pragmatic inference acquisition likely follows the order identified in previous research, but appears to be completed at a later age in this language. 
This interesting combination of findings – expected hierarchy of difficulty, but differing age of acquisition – is an important addition to our growing cross-linguistic knowledge of SI and could be the result of language-specific differences in the use of SI lexical items or methodological differences. 
Utilization of busted CFL in developing cheap and efficient segmented compact LED bulbs 
N S Andres and R T Ponce 
Electrical Engineering Department, Bataan Peninsula State University, 2100, Balanga City Bataan, Philippines 
Abstract. 
Today’s generation will not survive a day without the help of lighting. 
In fact, someone’s productivity, particularly at night, depends on the presence of a good lighting and it seems that it is a daily necessity. 
Lighting takes a large part on the consumption of household electrical energy particularly in the Philippines. 
There are different type of lighting bulbs used at home can affect the overall lighting consumption. 
Nowadays, most commonly and widely used bulb in the household is the Compact Fluorescent Light (CFL). 
However, the main problem of CFL is the mercury they contain. 
In addition to this is the harmful effect of mercury such as Emission of UV Radiation. 
In response to the said problem, this project study gives solution to the problem of the society concerning environment, health and safety as well energy conservation, by developing a segmented compact light-emitting diode (SCLED) bulb from busted CFL that are efficient, economical, and does not contain toxic chemicals. 
1. Introduction 
Based on the results of the 2011 Household Energy Consumption Survey (HECS), electricity remains as the most common source of energy used by households particularly in the Philippines. 
About 87 percent of 21.0 million households used electricity from March to August 2011. 
As of 2016, the demand for electricity in the residential is leading among the different sectors. 
It is often grouped depending on its uses that are distributed for heating and cooling, lighting, operating appliances etc [1] 
Figure 1. 
January-june 2016 philippine power demand 
Now that the world is in the age where lighting seems to be a daily necessity, typical homes as shown in figure 1, consume nearly 27 percent of the energy used today: making lighting as the major source of electricity consumption. 
Lighting plays a large part on the consumption of household electrical energy; it consumes about 18% of the total generated electricity of total energy consumption in residential sectors as shown in figure 2 [2]. 
Figure 2. 
Estimated electricity use in residential sector 
One of the main factors in lighting energy consumption is the light bulb. 
The use of correct and appropriate type of light bulb improves the efficiency of energy usage. 
There are three general types of lighting that are widely used in the household nowadays: the (light-emitting diode) LED bulb, (compact fluorescent light) CFL bulb and the incandescent bulb. 
But each of them has their perks and perils [3]. 
Unfortunately, most of the time, people in the residential end up choosing a lamp based on it is price rather than its efficiency and this cause us to pay for our lighting more than necessary. 
Today, CFLs are the most used lighting sources in the household. 
Using CFLs will allow people to decrease their energy consumption; also it is a good start to decrease greenhouse emissions. 
However, the main problem of CFLs is the mercury they contain. 
When products and wastes containing mercury are improperly disposed of, mercury is released into the air, ground or water. 
It is persistent in the environment; it never breaks down nor goes away. 
In addition to the harmful effects of mercury is that it emits Ultraviolet (UV) Radiation. 
This UV radiation interacts with the chemicals on the inside of the bulb to generate light. 
The acute and chronic effects are the normal responses of the skin to UVR; acute reactions considered will be erythema (sunburn) and vitamin D production. 
Skin aging and skin cancer will be discussed as those reactions produced by prolonged or repeated UVR exposure [4]. 
In response to the aforementioned situation where the three types of lights are compared by their efficiency and effectiveness as light sources, the proponents conducted this study. 
Since the CFLs are the most commonly used light bulbs in the residential and has a great compatibility to LED in terms of design and materials, the proponents conducted a study about innovating and recycling CFL into a more efficient and environment friendly LED light bulb. 
Design of Mutation Operators for Testing Geographic Information Systems 
Suilen H. Alvarado Laboratorio de Bases de Datos Campus de Elviña, Centro de investigación CITIC, Universidade da Coruña, 15071 A Coruña, Spain; s.hernandez@udc.es 
Presented at the 2nd XoveTIC Congress, A Coruña, Spain, 5–6 September 2019. 
Abstract: 
In this article, we propose the definition of specific mutation operators for testing Geographic Information Systems. 
We describe the process for applying the operators and generating mutants, and present a case study where these mutation operators are applied to two real-world applications. 
Keywords: mutation operators; geographic information systems; mutation testing 
1. Introduction 
Mutation-based testing [1] is a test technique that involves artificially introducing errors into a System Under Test (SUT). 
A mutant is a copy of the system in which a change has been done that, in most cases, will lead to a behaviour different than expected. 
The different mutants are generated automatically by the application of mutation operators. 
In the state of the art, we have found mutation operators, both general purpose and specific to different technologies, languages and paradigms [2–9]. 
However, these operators are not adequate when trying to test software features associated with specific domains. 
In this article, we propose mutation operators specific to the domain of Geographic Information Systems (GIS) applications. 
These operators reproduce programming errors that are litely to occur during the development of this type of applications. 
In addition, we present the implementation of these operators and as proof of concept we apply these operators to two real-world GIS applications and we generate the mutants. 
2. Mutation Operators for GIS 
As a previous step to designing the mutation operators, we analyzed the main technologies used specifically in the development of GIS, and we identified typical errors a programmer can introduce during the development. 
These errors were formalized into mutation operators. 
In order to apply these operators to a SUT, we rely on Java reflection and aspect-oriented programming. 
Reflection allows us to obtain the list of classes and methods of the SUT, so the user can decide the methods to wish the operators will be applied. 
Later, we capture information about the methods of the SUT to be mutated, together with the information of the mutation operators that were already defined. 
From these data, we generate the mutation operator, in the form of on aspect, which will then be possible to interweave with the SUT which generates a mutant of the SUT. 
Next, we describe the definition of two operators and two cases of application on real-world GIS applications. 
ChangeCoordSys Operator (Listing 1): 
It exchanges the coordinate system of a geometry, so it does not match the coordinate system that is being used in the user interface. 
It simulates the error of not checking that the coordinate system is correct. 
The error is introduced by directly modifying the coordinate system of geometry when recovering the wrapping of the figure. 
Listing 1: A simplified definition of the ChangeCoordSys Operator. 
This operator was applied to a mobile technology GIS application. 
This application allows registering places of interest for the user. 
These areas of interest are called Geofences. 
A Geofence is determined by a geographical location expressed in terms of latitude, longitude, and a radius around that location. 
By creating a Geofence with an erroneous location from its central location, the device will receive incorrect location notifications. 
As a result, the user will see in the application’s map viewer the Geofences drawn in erroneous zones (Figure 1). 
Figure 1. Original and mutant application. 
BooleanPolygonConstraint Operator (Listing 2): 
It introduces errors in the processing of geometries, manipulating the result of the operations that carry out the verification of different topological restrictions between geometries, such as intersects, covers or overlap. 
Listing 2: A simplified definition of the BooleanPolygonConstraint Operator. 
To test this operator it was applied to a land reparcelling system. 
The objective of the land reparcelling is to reunify the lands of an owner to facilitate their exploitation. 
In this application, the result of the operation between two polygons has been affected. 
This error causes the incorrect display of the resulting geometry that should be drawn in the user interface after the operation applied to the two initial geometries (Figure 2). 
Figure 2. Original and mutant application. 
3. Conclusions 
In existing proposals, we can find both generic and specific mutation operators. 
However, these are not adequate to cover errors in particular domains. 
We have defined new operators specific to the GIS domain and a way to apply them to a SUT. 
In addition, we have tested the operators defined in two GIS applications. 
As future work, we intend to extend this approach to other domains, as well as to use the developed operators for the automatic improvement of sets of test cases. 
Replication in Second Language Research: 
Narrative and Systematic Reviews and Recommendations for the Field 
Replication studies are considered by many to play a fundamental role in any scientific endeavor. 
When using the same materials and procedures as a previous study, replication studies serve to test the reliability of the previous study’s findings. 
When altering specific methodological or participant characteristics of a previous study, they serve to test generalizability of the earlier findings under different conditions. 
One indication of the importance of replication is found in the 50 or more calls for replication research in the field of second language (L2) research alone (see references for 50 calls and commentaries in Appendix S1 in the Supporting Information online): from Santos (1989) through Polio and Gass 1997 to very recent proposals for specific replication studies, such as Vandergrift and Cross 2017 and even a book-length treatment (Porte, 2012). 
Beyond these calls, efforts to actively promote and facilitate replication studies have also emerged. 
For example, the Instruments for Research into Second Languages (IRIS) repository (http://www.irisdatabase.org) was established in 2011 and holds, at the time of writing, over 3,800 materials that can be used for replication, among other purposes, in L2 research (Marsden & Mackey, 2014; Marsden, Mackey, & Plonsky, 2016). 
The Open Science Framework (https://osf.io), also established in 2011, provides a web infrastructure to facilitate collaboration and has been used for large replication efforts in psychology (e.g., Open Science Collaboration, 2015), which continue to make waves in academia (Laws, 2016; Lindsay, 2015; Martin & Clarke, 2017) and the general media (Baker, 2015; Devlin, 2016). 
In some fields, a flourishing metascience, that is, the scientific study of science (see Munafò et al., 2017), has included syntheses assessing the quantity and nature of replication efforts, for example, in education (Makel & Plucker, 2014 and in psychology (Makel et al., 2012). 
The driving force behind this battery of calls, commentaries, infrastructure, and metascience is a perceived crisis in the state of replication research. 
The severe concerns underpinning the alleged crisis have several dimensions relating to: (a) the (small) amount of published replication research; (b) the (poor) quality of replication research; and (c) the (lack of) reproducibility, which refers to the extent to which findings can (not) be reproduced in replication attempts that have been undertaken. 
These concerns speak to the very core of science, raising fundamental questions about the validity and reliability of our work. 
Indeed, some commentators have called replication the “gold standard” of research evidence (Jasny, Chin, Chong, & Vignieri, 2011, p. 1225) and a “linchpin of the scientific process” (Let’s replicate, 2006, p. 330). 
In the field of L2 research, given the importance of replication and the 50 calls for replication in L2 research that we identified, one might expect a substantial number of published replication studies by now. 
However, a perceived lack of prestige, excitement, and originality of replication plagues L2 research (Porte, 2012), as it does other disciplines (Berez-Kroeker et al., 2017; Branco, Cohen, Vossen, Ide, & Calzolari, 2017; Chambers, 2017; Schmidt, 2009), and these perceptions are thought to have caused, at least in part (directly or indirectly), alleged low rates and a poor quality of published replication studies. 
However, a systematic metascience on replication research has not yet been established in the field of L2 research, leaving a poor understanding of the actual number and nature of replication studies that have been published. 
The current study begins to address this gap through narrative and systematic reviews. 
The narrative review considers challenges in replication research and is largely informed by commentaries and metascience from psychology, given that the cognitive and social subdomains of psychology are highly influential in L2 research, and also from education, another key sister discipline. 
The narrative review is organized around four broad themes: (a) the quantity of replication research, (b) the nature of replication research, (c) the relationship between initial and replication studies, and (d) the interpretation and extent of reproducibility of the findings of initial studies. 
To gain insight into these issues in the context of L2 research, the systematic review provides a synthesis of L2 studies in journal articles that self-labeled as replications. 
The research questions and methods of the systematic review were largely determined by the narrative review but also emerged through the design and piloting of the coding instrument. 
Finally, we offer further discussion and 16 recommendations for future replication work that draw on our narrative and systematic reviews and on our experience of carrying out multisite (Morgan-Short et al., 2018) 1 and single site (Faretta-Stutenberg & Morgan-Short, 2011; Marsden, Williams, & Liu, 2013; McManus & Marsden, 2017; Morgan-Short, Heil, Botero-Moriaty, & Ebert, 2012) replications. 
We start from the widely agreed premise that testing the reproducibility of findings should have an essential role in the testing and refinement of theory, at least for hypothesis-testing epistemologies that seek to ascertain generalizability and for other epistemologies in which constructs are deemed to be definable and observable. 
Thus, our overall aim is to provide conceptual clarification and an empirical base for future discussion and production of replication studies, with a view to improving the amount and quality of L2 replication research. 
1. Introduction 
Salinity is one of the most important marine parameters, which controls many processes such as physical circulations, biogeochemistry dynamics from regional to global ocean [1,2]. 
Although drifters and buoys, together with cruises, have accumulated large amount in-situ water salinity data in different regions, it is still difficult to monitor global ocean salinity with high temporal-spatial resolution by the in situ measurements. 
In the past decade, with the successful launch of the soil moisture and ocean salinity (SMOS) satellite by the European Space Agency (ESA) [3], the Aquarius/SAC-D satellite [4], and the soil moisture active passive (SMAP) satellite [5,6] by the National Aeronautics and Space Administration (NASA), global sea surface salinity (SSS) observations from space have become possible and a significant improvement has been made to understand the ocean dynamics and climate change. 
The early concept of remote sensing of SSS has been demonstrated in the late 1970s with observations by Skylab [7] and two airborne L-band radiometry experiments [8,9]. 
At the end of 1990, two airborne microwave interferometers, the electronically scanned thinned array radiometer (ESTAR) and the scanning low-frequency microwave radiometer (SLFMR), successfully produced SSS maps in coastal areas in agreement with in-situ measurements with an accuracy of about 1 psu. 
Based on many experiments, the L-band is evidenced as the optimal frequency for remote sensing of SSS, which has been adopted by SMOS, Aquarius/SAC-D and SMAP. 
However, the sensitivity of satellite measured brightness temperature to SSS is quite low. 
For example, the sensitivity of vertically polarized brightness temperature to SSS variation is 0.4 to 0.8 K/psu for different observing angles and sea surface temperatures (SST), and it is only 0.2 to 0.6 K/psu for the horizontal polarization brightness temperature [10]. 
Thus, remote sensing of SSS requires a highly accurate retrieval model. 
It is widely accepted that the corrections of the sea surface and atmospheric effects are essential for remote sensing of SSS, since these effects could alter the value of sensor-measured brightness temperature and introduce errors into the SSS retrieval process. 
Besides the atmospheric effects, the increasing of sea surface emissivity due to the sea surface roughness and foam effects is the main source of error, which could significantly hamper the accuracy of SSS retrieval [11]. 
Over the past decades, the correction for sea surface roughness effects were studied based on the in-situ and airborne measurements; for example, the experiments made from a tower [12], wind and salinity experiments (WISE) [13,14], airborne Passive-Active L-band Sensor (PALS) campaign [15] and Combined Airborne Radio instruments for Ocean and Land Studies (CAROLS) campaigns [16,17]. 
Many rough surface emission models have also been developed based on the theoretical and empirical methods. 
Among these models, the small-slope approximation/small perturbation model (SSA / SPM) [18,19,20,21], two scale model (TSM) [22,23,24] and empirical/semi-empirical models [25,26] have been widely used by the research community and implemented in different satellite data processing systems. 
As the foam effect is significant at high wind speed conditions (above a threshold of 12 m/s) due to strong wave breaking, it has been corrected by numerous models; for example, the semi-empirical models [27,28] and radiative transfer equation (RTE) based models [29,30], which were developed to estimate the foam covered sea surface emissivity. 
Although many theoretical and empirical models have been developed, some problems are still unsolved. 
For example, the TSM originally proposed to estimate brightness temperature at higher frequencies, uses the sea surface wave spectrum by multiplying a factor of 2. 
However, whether this modification can be applied to L-band is still unclear, and the choice of cutoff wavenumber is arbitrary and needs to be clarified in the L-band. 
Moreover, the sea surface reflection of downwelling atmospheric emission is another contribution to satellite-observed brightness temperature [31], thus the determination of the cutoff wavenumber is required not only for sea surface emission but also for reflection. 
In addition, the widely used empirical models decouple the wind effect from SSS and SST effects, which means that the surface emission is due to a perfectly flat sea surface and the wind-roughened sea surface. 
The wind-roughened sea surface is associated with the increased brightness temperature (due to sea surface roughness effect), which induces regional biases when applied to different areas [32]. 
Furthermore, the satellite measurements of SSS are hampered by the effect of radio frequency interference (RFI) in offshore areas of China (i.e., Bohai sea, Yellow sea and East China sea), which causes a large amount of data to be discarded [33]. 
Thus, the compatibility of these models in coastal area of China needs to be assessed and tested in order to achieve higher accuracies of SSS from the space-borne observations. 
Discussion 
We have shown that the addition of an artificial tail during ontogeny can produce postural and locomotory changes in chickens, consistent with the posture and kinematics inferred for non-avian dinosaurs [5], [6], [11]. 
The posterior displacement of the CoM produced a more vertically oriented femur during standing (femur in experimental animals was 40% more vertical than control subjects), and increased femoral retraction and decreased knee flexion during walking. 
These results indicate a shift from the standard bird, knee-driven bipedal locomotion to a more hip-driven locomotion, typical of crocodilians (the only other extant archosaur group), mammals, and hypothetically, bipedal non-avian dinosaurs. 
These postural and kinematics changes cannot be attributed to an increased weight as subjects of the control-weight group did not show the same changes as the experimental group. 
In fact, the control-weight subjects showed a more horizontally oriented femur during walking with respect to the control group, similar to that observed in Carrano and Biewener's experimental subjects [7]. 
Therefore, we conclude that the location of the CoM can be a key factor in defining limb posture and kinematics. 
It has been proposed that the relative mass of the CFL can be used as a proxy to estimate the relative importance of femoral retraction during locomotion in extinct bipedal dinosaurs [8]. 
Our data show that for a given CFL mass, femoral retraction can be greatly affected by the location of the CoM and limb postures. 
Furthermore, limb retraction can be markedly modulated with speed [5], suggesting caution when using simple morphological parameters to estimate functional relationships. 
Differences in limb orientation can produce substantial differences in loading regimes on limb bones. 
The orientation of each limb element to the ground reaction force (GRF) indicates the relative contribution of axial and bending forces to external bone loading: a bone perpendicular to the GRF is expected experience greater bending forces than one parallel to the GRF. 
Because bone adapts to its loading environment [19], [20], [21], geometric information from limb bones, such as lengths and cross-sectional geometry, are expected to reflect differences in loading regimes and consequently in behavior and locomotor patterns [22], [23]. 
In this framework, scaling differences in femoral geometry between non-avian theropods and birds have been suggested to be the result of postural differences between these groups [6], [23]. 
Birds have relatively shorter, stouter femora than non-avian theropods, presumed to be associated with more horizontal orientation. 
Experimental manipulations of femoral orientation in chickens suggest that torsional loads increase as the femur becomes more horizontal [7] supporting the idea that postural differences could be reflected in differences in limb cross-sectional geometry. 
To test if the postural differences observed in this study produced changes in limb morphology, we measured the length and mid-shaft cross-sectional properties of the femur in all our individuals. 
However, we found no differences in cross-sectional femoral geometry among groups. 
Maybe this is not surprising considering that a recent study analyzing the relationship between posture and femur cross-sectional properties failed to find differences between birds and non-avian theropods [24], suggesting that simple morphological correlates of limb posture should be used with caution. 
Interestingly, femur length tended to be greater in the experimental group than in both the control-weight and the control group (by 4 and 7%, respectively), although not signifcant. 
Longer limbs are expected to experience larger bending and torsional moments, so the fact that experimental animals had longer femora suggests that limb verticalization reduces these moments by orienting the bone more parallel to the GRF line of action. 
If this were the case, it would support the idea that non-avian theropods have relatively thinner femora than extant birds because of postural differences [6]. 
The present study was inspired by Carrano & Biewener [7] but our results differed markedly from theirs. 
We suggest that the different outcomes are due to the distinct rearing and exercising conditions used in each study, in addition to the different artificial tails used. 
First, our experimental subjects lived in a large enclosure under conditions that allowed them to exercise all day long. 
In Carrano & Biewener's study, experimental chickens were housed individually in smaller cages and were only allowed to exercise 20 minutes per day, 3 days per week, from the 6th to the 12th week. 
Second, in their study, a lead mass was attached at the distal end of the experimental tail, probably generating excessive displacement of the CoM. 
During avian evolution, the loss of the CFL and reorganization of the pelvic musculature [5], [13] could have made birds unable to properly carry a postacetabular mass equivalent to that carried by non-avian theropods [5], [6]. 
In our experimental setup, we attempted to more closely mimic non-avian theropod tail morphology, in which mass is distributed through a distally tapering tail. 
In addition, we reduced the total tail mass to 15% body mass from the 20% body mass used by Carrano and Biewener. 
Thus, our study seems to have generated a more gradual and less pronounced change in the moment of inertia produced by the artificial tail, allowing experimental subjects to adjust to the posterior mass by adopting a more vertical position of the femur while standing. 
Interestingly, the femur kinematics during walking in our control-weight group resembles the results reported in the experimental subjects of Carrano and Biewener. 
This suggests that their results could be partially explained as a response to the increased loading rather than to the displacement of the CoM. 
Due to the phylogenetic relatedness, extant birds have been used to inform functional aspects of non-avian dinosaur locomotion. 
However, substantial differences in hindlimb morphology between these groups make difficult to assess the validity of inferences obtained from such studies. 
It has even been proposed that, due to functional convergence, mammals might be a better system to study bipedal dinosaur locomotion [7], [23], but the results reported here show that important aspects of non-avian theropod locomotion can be experimentally recreated in modern birds. 
One caveat, however, is that our approach uses tail reduction as the mechanism for CoM displacement despite it has been recently shown that the evolutionary change in CoM position was driven instead by forelimb enlargement [8]. 
Nonetheless, this does not mean that tail reduction had no effect on CoM displacement, but that it was not the most important factor. 
Ideally we would have increased tail mass and reduced pectoral limb mass but, unfortunately, this is not experimentally feasible. 
We argue that our experimental approach, although not perfect, was effective in displacing the CoM and recreating locomotor patterns expected in non-avian theropods. 
Thus, we expect that careful phenotypic manipulation of extant birds can open new avenues of experimental investigation into unexplored facets of dinosaur locomotor mechanics and energetics, providing a more nuanced understanding of the relationship between form and function in dinosaur evolution. 
Re(a)d Wedding: 
A Comparative Discourse Analysis of Fan Responses to Game of Thrones 
It is no exaggeration to say that HBO’s Game of Thrones is more than just a television series or a successful brand: 
it is a transmedia system in the sense first used by Marsha Kinder (1991) and popularized by Henry Jenkins (2006), in which media-hopping networks of intertextualities extend the “storyworld” of an original production. 
Now spanning six seasons and 60 episodes, with an average global viewership (from its most recent season) of 25.1 million viewers per episode (Shepherd, 2016), it has spawned five video games, a graphic novel adaptation, several companion books, two rap albums, a 28-city orchestral tour, a wide variety of tabletop games, toys, merchandise and mobile apps, and countless podcasts, fanfics and other fan-based creations. 
Given the volume of content this represents, it is easy to forget that the television series itself is an adaptation of a book series with a pre-existing fandom. 
As such, the Game of Thrones storyworld represents a remarkably rich and challenging environment for fans old and new, who must negotiate an increasingly complex network of paratexts and intertexts in order to fully engage with its narratives. 
In this sense, fans of the series represent an emerging model for cultural consumption that should be carefully explored. 
Transmedia systems, like that exemplified by Game of Thrones, are becoming increasingly prevalent (e.g., Star Wars, Harry Potter, The Walking Dead, the Marvel Cinematic Universe, etc); these systems demonstrate, in microcosm, the global challenge of managing the fire-hose flow of information in contemporary postdigital society. 
The study of how people, as fans, access and manage information within a transmedia system provides valuable insight that contributes not only to practitioners and scholars of the media industry, but to the wider context of cultural studies, by offering findings on this new model of the fan as consumer and information-user. 
For us, as digital humanists, defining the “transmedia fan” is of particular relevance as we seek to understand contemporary social and cultural transformations engendered by digital technologies. 
Methodology 
As a first step in defining the “transmedia fan”, the current project undertakes a comparative discourse analysis of online conversations of Game of Thrones fans. 
One of the most dramatic plot developments in the source material (Martin, 2000) was adapted to the screen in the penultimate episode of the third season, “The Rains of Castamere” (Benioff & Weiss, 2013). 
Readers of the book series had long anticipated and dreaded the events of the “Red Wedding”, while fans of the show unfamiliar with Martin’s narrative were largely taken unawares by the pivotal episode. 
Since the television series’ inception, writers at The AV Club have written two critical reviews for each episode: 
one for viewers familiar with the books (i.e., “Experts”) and one for viewers unfamiliar with the books and averse to “spoilers” (i.e., “Newbies”). 
What results are two completely separate reviews of “The Rains of Castamere” which in turn document the fans’ reactions to the episode in the form of user comment threads: 
one comment thread where fans were expected to be shocked by the outcome of the episode and one comment thread where fans had hotly anticipated it. 
As a pilot project, the current work takes the content of both comment threads — a corpus of approximately 5,600 comments — and analyzes each thread separately using a qualitative coding method aligned with constructivist grounded theory (Charmaz, 2006). 
Through this analysis, a categorization of themes emerges illustrating tactics for negotiating intertexts and paratexts unique to each group of fans. 
These themes fall under two broad categories: 
negotiation (i.e., emotional responses) and tactical negotiation (i.e., cognitive, or reasoned responses). 
A comparison of categories and sub-categories between both groups provides preliminary findings to support an emergent model, or models, of the “transmedia fan”. 
Conclusion 
The present research represents a first step in exploring the impact of transmedia systems, as exemplified by Game of Thrones, through the study of fans. 
The question posed by this research is, fundamentally, an examination of how the problem of “access” is framed in postdigital society from the perspective of the consumer. 
Future research should explore the negotiation tactics observed in transmedia fans using the principles of De Certeau’s (1984) everyday life practice, in order to extend its application to the broader context of modern-day consumers. 
The current study will contribute to the development of further qualitative and quantitative research that will more clearly define the information behaviors of the transmedia fan. 
This project is of relevance to researchers in media studies, fan studies, information studies and digital humanities. 
